{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d5346a-9532-41c3-ba4b-cff67fd7338f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502c98c6-c610-48a4-af0d-c67b09bde0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets==1.18.4 accelerate==0.20.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7aed0d-7f59-4b1e-a3e1-1975fab51e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.debugger import ProfilerConfig, DebuggerHookConfig, Rule, ProfilerRule, rule_configs\n",
    "import sagemaker.huggingface\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from textwrap import wrap\n",
    "\n",
    "import boto3\n",
    "import pprint\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97cb4d4-7302-4f30-bd1c-80b255287e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#import the data and do required transformations\n",
    "\n",
    "bucket='llmtraining'\n",
    "data_key1 = 'LLM_TrainingData_071423.csv'\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "obj_props = s3.get_object(Bucket = bucket, Key=data_key1)\n",
    "df_orig = pd.read_csv((obj_props['Body']), index_col=False)\n",
    "df_orig = df_orig.drop(columns=['Unnamed: 0'])\n",
    "df_orig.dropna(inplace=True)\n",
    "df_orig.info()\n",
    "df_orig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571ce3e3-7a5d-4ce7-bce4-4bb19202794a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_orig\n",
    "keep_cols = ['text', 'labels']\n",
    "df2 = df[keep_cols]\n",
    "#df2 = df2.rename(columns={'summary': 'text', 'code': 'labels'})\n",
    "df2.info()\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a6c3d1-4274-4de5-a7f4-0e545bfe6017",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a huggingface dataset object, and encode the class labels (convert from str to int, while saving a dictionary)\n",
    "\n",
    "df_dataset = Dataset.from_pandas(df2)\n",
    "df_dataset = df_dataset.class_encode_column('labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423fcc24-87bd-430d-9ed8-35e1c9a7ca58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test train split using datasets functionality\n",
    "\n",
    "df_dataset = df_dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = df_dataset['train']\n",
    "test_dataset = df_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37d03ef-3c9f-43a5-aef7-1c0a7909edf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer used in preprocessing\n",
    "model_id = 'allenai/scibert_scivocab_uncased'\n",
    "\n",
    "# dataset used\n",
    "dataset_name = 'sample_data'\n",
    "\n",
    "# s3 key prefix for the data\n",
    "s3_prefix = 'samples/datasets/scibert_multiclass'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8358fde3-1b09-4c6b-b62a-c3586fa4fa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, model_max_length=512)\n",
    "\n",
    "# tokenizer helper function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True)\n",
    "\n",
    "# tokenize dataset\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "# set format for pytorch\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e767ac1b-ae9b-4986-9fb9-11ccae5389dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "s3 = S3FileSystem()  \n",
    "\n",
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train'\n",
    "train_dataset.save_to_disk(training_input_path, fs=s3)\n",
    "\n",
    "# save test_dataset to s3\n",
    "test_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test'\n",
    "test_dataset.save_to_disk(test_input_path, fs=s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651fa7a5-ad7b-494a-867c-737ba85021c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Uploaded training data to {training_input_path}')\n",
    "print(f'Uploaded testing data to {test_input_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f2962a-095f-4ff3-9fdf-82cd554757b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# START HERE IF DATA IS ALREADY IN S3\n",
    "\n",
    "model_id = 'allenai/scibert_scivocab_uncased'\n",
    "s3_prefix = 'samples/datasets/scibert_multiclass'\n",
    "dataset_name = 'sample_data'\n",
    "training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train'\n",
    "test_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d44ab4f-52e7-4d98-9afc-ebe214512444",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace, TrainingCompilerConfig\n",
    "\n",
    "# initialize the Amazon Training Compiler\n",
    "compiler_config=TrainingCompilerConfig()\n",
    "\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={'epochs': 3,                                    # number of training epochs\n",
    "                 'train_batch_size': 20,                         # batch size for training\n",
    "                 'eval_batch_size': 24,                          # batch size for evaluation\n",
    "                 'learning_rate': 3e-5,                          # learning rate used during training\n",
    "                 'model_id':model_id,                            # pre-trained model\n",
    "                 'fp16': True,                                   # Whether to use 16-bit (mixed) precision training\n",
    "                }\n",
    "\n",
    "# job name for sagemaker training \n",
    "job_name=f\"scibert-{dataset_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feba0034-53ec-430f-a43d-2c95251f6cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions=[\n",
    "    {'Name': 'loss', 'Regex': \"'loss': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'learning_rate', 'Regex': \"'learning_rate': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_loss', 'Regex': \"'eval_loss': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_accuracy', 'Regex': \"'eval_accuracy': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_runtime', 'Regex': \"'eval_runtime': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_samples_per_second', 'Regex': \"'eval_samples_per_second': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'epoch', 'Regex': \"'epoch': ([0-9]+(.|e\\-)[0-9]+),?\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5090b7d-9ac5-46b9-ad04-ea34a40d39e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'train.py',        # fine-tuning script used in training jon\n",
    "    source_dir           = './scripts',       # directory where fine-tuning script is stored\n",
    "    instance_type        = 'ml.p3.2xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    transformers_version = '4.11.0',          # the transformers version used in the training job\n",
    "    pytorch_version      = '1.9.0',           # the pytorch_version version used in the training job\n",
    "    py_version           = 'py38',            # the python version used in the training job\n",
    "    hyperparameters      = hyperparameters,   # the hyperparameter used for running the training job\n",
    "    compiler_config      = compiler_config,   # the compiler configuration used in the training job\n",
    "    disable_profiler     = True,              # whether to disable the profiler during training used to gain maximum performance\n",
    "    debugger_hook_config = False,             # whether to enable the debugger hook during training used to gain maximum performance\n",
    "    metric_definitions   = metric_definitions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38c3846-a9fa-4030-a052-5fa8f2a31a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {\n",
    "    'train': training_input_path,\n",
    "    'test': test_input_path\n",
    "}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227f4708-79e8-47f2-b9c2-ffd3ae17bf2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa98da59-9884-441c-9f86-fb858e01d2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import TrainingJobAnalytics\n",
    "\n",
    "df_metrics = TrainingJobAnalytics(training_job_name=huggingface_estimator.latest_training_job.name).dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242d81d7-b060-4f2c-8e4a-121e9c7bf761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_wide = pd.pivot(df_metrics, index=['timestamp'], columns=['metric_name'], values='value')\n",
    "df_wide.reset_index(inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
